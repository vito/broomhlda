use(require("atomy"))

token = require(File expand-path("../token", __FILE__))


Error data(NoMatchFor(@input))

def(NoMatchFor(input) message):
  "no matchers succeeded for: " +
    (input slice(0, 50) inspect + " [...]")


data(Matcher(@regexp, @type, @next))

-- TODO: Using
fn(tokens(data, gs & token ByGroups)):
  gs types zip(data to-a drop(1)) [t, d]:
    when(t nil? || d nil?):
      return(nil)

    @lexed << token Token new(t, d)

fn(tokens(data, t & token Tagged)):
  @lexed << token Token new(t, data to-s)

-- go through matchers until one matches the immediate
-- next input. if none match, try again, but allowing
-- matches past the start.
fn(try-all(matchers)):
  closest = nil
  worked = nil

  matchers each [m]:
    d = m regexp match(@input)
    when(d && (!closest || (d pre-match size < closest pre-match size))):
      closest =! d
      worked =! m

      when(closest pre-match empty?):
        break

  unless(closest):
    /error(NoMatchFor new(@input))

  skipped = closest pre-match
  @input = closest post-match

  worked next apply(@state, closest)

  unless(skipped empty?):
    @lexed <<
      token Token new(
        token Tagged new("err")
        skipped)

  tokens(closest, worked type)

lexer = class:
  def(initialize(input)):
    @input = input dup

    @state = [
      class matchers[class info[.start]][]
    ] -- [Matcher]

    @lexed = [] -- [Token]

  -- run the lexer until the input is empty, yielding
  -- the lexed tokens
  def(run):
    until(@input empty?):
      try-all(@state last)

    @lexed

  singleton:
    attr-accessor(.matchers, .info)

    def(lex(x)): new(x) run

    def(name &x):
      @info[.name] = x call

    def(aliases &x):
      @info[.aliases] = x call

    def(extensions &x):
      @info[.extensions] = x call

    def(mimetypes &x):
      @info[.mimetypes] = x call

    def(start &x):
      @info[.start] = x call

    def(flags &x):
      @info[.flags] = x call

const-set(.Lexer, lexer)
